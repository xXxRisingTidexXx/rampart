{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8svmUftZOtL"
   },
   "source": [
    "![logo.png](../images/logo.png)\n",
    "\n",
    "Auge is an image classification model. Its main target's to recognize common photos to determine a few flat publication's features. Each image belongs to a specific realty, having a bunch of photo & panorama recognized **twinkle** can better predict apartments' order.\n",
    "\n",
    "## I/O\n",
    "Required images lie at `../scientific/images`. Final classifier must be stored into `../models/auge.latest.pth`.\n",
    "\n",
    "## Metadata\n",
    "Each image file has a self-explained name looking like:\n",
    "```\n",
    "<hash>.<effect>.<group>.<label>.webp\n",
    "```\n",
    "For instance:\n",
    "```\n",
    "0018559490dd9cb73caa00f078fde40b220803de.balance_down_hue_cw_rotate_ccw_crop.training.construction.webp\n",
    "```\n",
    "Placeholders in angle brackets:\n",
    "- `hash`, SHA-1 sum of the image URL.\n",
    "- `effect`, image filters used to augment the initial photo. An unchaged file has `origin` here.\n",
    "- `group`, the dataset image belongs to. Can be one of `training`, `validation` & `testing` .\n",
    "- `label`, expected image class. See the details below.\n",
    "\n",
    "## Classes\n",
    "- `luxury` is a flat with rich furniture, huge rooms, chandeliers, fireplaces, etc.\n",
    "\n",
    "![luxury1.webp](../images/luxury1.webp)\n",
    "![luxury2.webp](../images/luxury2.webp)\n",
    "![luxury3.webp](../images/luxury3.webp)\n",
    "![luxury4.webp](../images/luxury4.webp)\n",
    "\n",
    "- `comfort` is the most suitable for an ordinary citizen apartments. Clean, neat, sometimes minimalistic, average area, qualitive furniture, etc.\n",
    "\n",
    "![comfort1.webp](../images/comfort1.webp)\n",
    "![comfort2.webp](../images/comfort2.webp)\n",
    "![comfort3.webp](../images/comfort3.webp)\n",
    "![comfort4.webp](../images/comfort4.webp)\n",
    "\n",
    "- `junk` is an old flat image. Probably, the whole apartments should belong to a dormitory, Khrushchevka or gostinka.\n",
    "\n",
    "![junk1.webp](../images/junk1.webp)\n",
    "![junk2.webp](../images/junk2.webp)\n",
    "![junk3.webp](../images/junk3.webp)\n",
    "![junk4.webp](../images/junk4.webp)\n",
    "\n",
    "- `construction` is a flat without a finished design. No doors, floor, supplies, wallpapers, ceiling, furniture, etc. Typically, new buildings contain these apartments.\n",
    "\n",
    "![construction1.webp](../images/construction1.webp)\n",
    "![construction2.webp](../images/construction2.webp)\n",
    "![construction3.webp](../images/construction3.webp)\n",
    "![construction4.webp](../images/construction4.webp)\n",
    "\n",
    "- `excess` is the trash category. Actually, all exterior photos, outlines & posters lie here.\n",
    "\n",
    "![excess1.webp](../images/excess1.webp)\n",
    "![excess2.webp](../images/excess2.webp)\n",
    "![excess3.webp](../images/excess3.webp)\n",
    "![excess4.webp](../images/excess4.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPF82oNZZOtL"
   },
   "outputs": [],
   "source": [
    "from plotly.graph_objs import Pie, Figure, Scatter\n",
    "from plotly.figure_factory import create_annotated_heatmap\n",
    "from plotly.subplots import make_subplots\n",
    "from re import match\n",
    "from numpy import arange, trace, sum\n",
    "from glob import glob\n",
    "from pandas import DataFrame, concat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torchvision.models import inception_v3\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch import no_grad, save, max, load, zeros, cat, long, set_num_threads, get_num_threads\n",
    "from uuid import uuid4\n",
    "from PIL.Image import open\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "set_num_threads(cpu_count())\n",
    "print(f'Set thread number to {get_num_threads()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8y1Ga1pjZOtN"
   },
   "outputs": [],
   "source": [
    "groups = {'training', 'validation', 'testing'}\n",
    "interiors = ['luxury', 'comfort', 'junk', 'construction', 'excess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    mappings = {l: i for i, l in enumerate(interiors)}\n",
    "    return DataFrame(\n",
    "        map(lambda p: parse(p, mappings), glob('../scientific/images/*.webp')),\n",
    "        columns=['path', 'group', 'interior']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path, mappings):\n",
    "    result = match(r'^.*/\\w+\\.\\w+\\.(\\w+)\\.(\\w+)\\.webp$', path)\n",
    "    if not result:\n",
    "        raise RuntimeError(f'Got invalid path, {path}')\n",
    "    expressions = result.groups()\n",
    "    if expressions[0] not in groups:\n",
    "        raise RuntimeError(f'Got invalid group, {path}')\n",
    "    if expressions[1] not in mappings:\n",
    "        raise RuntimeError(f'Got invalid interior, {path}')\n",
    "    return path, expressions[0], mappings[expressions[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM4e4XoVZOtO"
   },
   "outputs": [],
   "source": [
    "images = extract()\n",
    "figure = Figure()\n",
    "counts = images['group'].value_counts()\n",
    "figure.add_trace(Pie(labels=counts.index, values=counts.values, name=''))\n",
    "figure.update_layout(legend={'x': 0.63})\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = make_subplots(\n",
    "    cols=len(groups),\n",
    "    specs=[[{'type': 'domain'}] * len(groups)],\n",
    "    subplot_titles=list(groups)\n",
    ")\n",
    "for i, group in enumerate(groups, 1):\n",
    "    counts = images[images['group'] == group]['interior'].value_counts().sort_index()\n",
    "    figure.add_trace(\n",
    "        Pie(labels=[interiors[j] for j in counts.index], values=counts.values, name=''),\n",
    "        row=1,\n",
    "        col=i\n",
    "    )\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XccA9INZOtP"
   },
   "outputs": [],
   "source": [
    "class Gallery(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self._data = data.values\n",
    "        self._transforms = Compose(\n",
    "            [ToTensor(), Resize((299, 299)), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._transforms(open(self._data[index][0])), self._data[index][2], self._data[index][0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYuzSQPtZOtP"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "training_loader = DataLoader(Gallery(images[images['group'] == 'training']), batch_size, True)\n",
    "validation_loader = DataLoader(Gallery(images[images['group'] == 'validation']), batch_size)\n",
    "testing_loader = DataLoader(Gallery(images[images['group'] == 'testing']), batch_size)\n",
    "print(len(training_loader), len(validation_loader), len(testing_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyuIm8bcYjix"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    network = inception_v3(num_classes=len(interiors), init_weights=True)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(network.parameters(), weight_decay=0.0001)\n",
    "    epoch_number = 8\n",
    "    epochs = arange(epoch_number)\n",
    "    training_losses = [0.0] * epoch_number\n",
    "    training_accuracies = [0.0] * epoch_number\n",
    "    auxiliary_losses = [0.0] * epoch_number\n",
    "    auxiliary_accuracies = [0.0] * epoch_number\n",
    "    validation_losses = [0.0] * epoch_number\n",
    "    validation_accuracies = [0.0] * epoch_number\n",
    "    for epoch in epochs:\n",
    "        network.train()\n",
    "        total = 0\n",
    "        for batch in training_loader:\n",
    "            outputs = network(batch[0])\n",
    "            optimizer.zero_grad()\n",
    "            training_loss = criterion(outputs[0], batch[1])\n",
    "            auxiliary_loss = criterion(outputs[1], batch[1])\n",
    "            (training_loss + 0.4 * auxiliary_loss).backward()\n",
    "            optimizer.step()\n",
    "            training_losses[epoch] += training_loss.item() * batch[0].size(0)\n",
    "            training_accuracies[epoch] += (max(outputs[0], 1)[1] == batch[1]).float().sum().item()\n",
    "            auxiliary_losses[epoch] += auxiliary_loss.item() * batch[0].size(0)\n",
    "            auxiliary_accuracies[epoch] += (max(outputs[1], 1)[1] == batch[1]).float().sum().item()\n",
    "            total += batch[0].size(0)\n",
    "        training_losses[epoch] /= len(training_loader)\n",
    "        training_accuracies[epoch] *= 100 / total\n",
    "        auxiliary_losses[epoch] /= len(training_loader)\n",
    "        auxiliary_accuracies[epoch] *= 100 / total\n",
    "        network.eval()\n",
    "        total = 0\n",
    "        with no_grad():\n",
    "            for batch in validation_loader:\n",
    "                output = network(batch[0])\n",
    "                validation_losses[epoch] += criterion(output, batch[1]).item() * batch[0].size(0)\n",
    "                validation_accuracies[epoch] += (max(output, 1)[1] == batch[1]).float().sum().item()\n",
    "                total += batch[0].size(0)\n",
    "        validation_losses[epoch] /= len(validation_loader)\n",
    "        validation_accuracies[epoch] *= 100 / total\n",
    "    state = network.state_dict()\n",
    "    save(state, f'../scientific/models/auge.{uuid4().hex}.pth')\n",
    "    save(state, '../scientific/models/auge.latest.pth')\n",
    "    reflect(epochs, training_losses, auxiliary_losses, validation_losses, 'Loss')\n",
    "    reflect(epochs, training_accuracies, auxiliary_accuracies, validation_accuracies, 'Accuracy')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect(epochs, training, auxiliary, validation, title):\n",
    "    figure = Figure()\n",
    "    figure.add_trace(Scatter(x=epochs, y=training, name='Training'))\n",
    "    figure.add_trace(Scatter(x=epochs, y=auxiliary, name='Auxiliary'))\n",
    "    figure.add_trace(Scatter(x=epochs, y=validation, name='Validation'))\n",
    "    figure.update_layout(title=title)\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbwrWZ7WZOtQ"
   },
   "outputs": [],
   "source": [
    "def test(network):\n",
    "    network.eval()\n",
    "    predicted = zeros(0, dtype=long)\n",
    "    actual = zeros(0, dtype=long)\n",
    "    with no_grad():\n",
    "        for batch in testing_loader:\n",
    "            predicted = cat([predicted, max(network(batch[0]), 1)[1].view(-1)])\n",
    "            actual = cat([actual, batch[1].view(-1)])\n",
    "    matrix = confusion_matrix(actual, predicted)\n",
    "    figure = create_annotated_heatmap(z=matrix, x=interiors, y=interiors, hoverinfo='skip')\n",
    "    figure.update_xaxes(title_text='Predicted')\n",
    "    figure.update_yaxes(title_text='Actual', autorange='reversed')\n",
    "    figure.update_layout(title=f'Accuracy: {trace(matrix) / sum(matrix) * 100:.2f}%')\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS7CfNd6aSqM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test(train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use(tag='latest'):\n",
    "    network = inception_v3(num_classes=len(interiors), init_weights=False)\n",
    "    network.load_state_dict(load(f'../scientific/models/auge.{tag}.pth'))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test(use())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "auge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
